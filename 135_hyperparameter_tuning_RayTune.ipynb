{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0jiDVUG8llC",
    "outputId": "28b49034-fa85-4c0f-a4ff-70b58238d8ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ray[data,serve,train,tune]\n",
      "  Downloading ray-2.22.0-cp310-cp310-manylinux2014_x86_64.whl (65.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (8.1.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (3.14.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (4.19.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (1.0.8)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (24.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (3.20.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (6.0.1)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (1.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (2.31.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (2.0.3)\n",
      "Collecting tensorboardX>=1.9 (from ray[data,serve,train,tune])\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (14.0.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (2023.6.0)\n",
      "Collecting uvicorn[standard] (from ray[data,serve,train,tune])\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp-cors (from ray[data,serve,train,tune])\n",
      "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting watchfiles (from ray[data,serve,train,tune])\n",
      "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi (from ray[data,serve,train,tune])\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting py-spy>=0.2.0 (from ray[data,serve,train,tune])\n",
      "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (3.9.5)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (2.7.1)\n",
      "Collecting colorful (from ray[data,serve,train,tune])\n",
      "  Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting virtualenv!=20.21.1,>=20.0.24 (from ray[data,serve,train,tune])\n",
      "  Downloading virtualenv-20.26.2-py3-none-any.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencensus (from ray[data,serve,train,tune])\n",
      "  Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (6.4.0)\n",
      "Collecting starlette (from ray[data,serve,train,tune])\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (0.20.0)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (1.63.0)\n",
      "Collecting memray (from ray[data,serve,train,tune])\n",
      "  Downloading memray-1.12.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from ray[data,serve,train,tune]) (1.25.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[data,serve,train,tune]) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[data,serve,train,tune]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[data,serve,train,tune]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[data,serve,train,tune]) (4.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[data,serve,train,tune]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[data,serve,train,tune]) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[data,serve,train,tune]) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[data,serve,train,tune]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[data,serve,train,tune]) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[data,serve,train,tune]) (4.11.0)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[data,serve,train,tune])\n",
      "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[data,serve,train,tune]) (4.2.2)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi->ray[data,serve,train,tune])\n",
      "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Collecting httpx>=0.23.0 (from fastapi->ray[data,serve,train,tune])\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->ray[data,serve,train,tune]) (3.1.4)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi->ray[data,serve,train,tune])\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->ray[data,serve,train,tune])\n",
      "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson>=3.2.1 (from fastapi->ray[data,serve,train,tune])\n",
      "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->ray[data,serve,train,tune])\n",
      "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->ray[data,serve,train,tune]) (3.7.1)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]->ray[data,serve,train,tune])\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]->ray[data,serve,train,tune])\n",
      "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]->ray[data,serve,train,tune])\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->ray[data,serve,train,tune])\n",
      "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]->ray[data,serve,train,tune])\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[data,serve,train,tune]) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[data,serve,train,tune]) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[data,serve,train,tune]) (0.18.1)\n",
      "Requirement already satisfied: rich>=11.2.0 in /usr/local/lib/python3.10/dist-packages (from memray->ray[data,serve,train,tune]) (13.7.1)\n",
      "Collecting textual>=0.41.0 (from memray->ray[data,serve,train,tune])\n",
      "  Downloading textual-0.62.0-py3-none-any.whl (552 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.3/552.3 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencensus-context>=0.1.3 (from opencensus->ray[data,serve,train,tune])\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: six~=1.16 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[data,serve,train,tune]) (1.16.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[data,serve,train,tune]) (2.11.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[data,serve,train,tune]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[data,serve,train,tune]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[data,serve,train,tune]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[data,serve,train,tune]) (2024.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->ray[data,serve,train,tune]) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->ray[data,serve,train,tune]) (1.2.1)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->ray[data,serve,train,tune])\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi->ray[data,serve,train,tune])\n",
      "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,serve,train,tune]) (1.63.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,serve,train,tune]) (2.27.0)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->fastapi->ray[data,serve,train,tune])\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi->ray[data,serve,train,tune]) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.2.0->memray->ray[data,serve,train,tune]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.2.0->memray->ray[data,serve,train,tune]) (2.16.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,serve,train,tune]) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,serve,train,tune]) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,serve,train,tune]) (4.9)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[data,serve,train,tune]) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[data,serve,train,tune]) (2.0.3)\n",
      "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[data,serve,train,tune]) (0.4.1)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->ray[data,serve,train,tune])\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[data,serve,train,tune]) (1.0.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,serve,train,tune]) (0.6.0)\n",
      "Installing collected packages: py-spy, opencensus-context, distlib, colorful, websockets, virtualenv, uvloop, ujson, tensorboardX, shellingham, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, watchfiles, uvicorn, starlette, httpcore, email_validator, typer, httpx, aiohttp-cors, textual, ray, opencensus, fastapi-cli, memray, fastapi\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.4\n",
      "    Uninstalling typer-0.9.4:\n",
      "      Successfully uninstalled typer-0.9.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-cors-0.7.0 colorful-0.5.6 distlib-0.3.8 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 memray-1.12.0 opencensus-0.11.4 opencensus-context-0.1.3 orjson-3.10.3 py-spy-0.3.14 python-dotenv-1.0.1 python-multipart-0.0.9 ray-2.22.0 shellingham-1.5.4 starlette-0.37.2 tensorboardX-2.6.2.2 textual-0.62.0 typer-0.12.3 ujson-5.10.0 uvicorn-0.29.0 uvloop-0.19.0 virtualenv-20.26.2 watchfiles-0.21.0 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"ray[data,train,tune,serve]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrdPrVHA8llD"
   },
   "source": [
    "\n",
    "# Ray Tune을 이용한 하이퍼파라미터 튜닝\n",
    "\n",
    "[Ray Tune](https://docs.ray.io/en/latest/tune.html) 은 분산 하이퍼파라미터 튜닝을 위한 업계 표준 도구입니다.\n",
    "Ray Tune은 최신 하이퍼파라미터 검색 알고리즘을 포함하고 TensorBoard 및 기타 분석 라이브러리와 통합되며 기본적으로\n",
    "[Ray 의 분산 기계 학습 엔진](https://ray.io/) 을 통해 학습을 지원합니다.\n",
    "\n",
    "이 튜토리얼은 Ray Tune을 파이토치 학습 workflow에 통합하는 방법을 알려줍니다.\n",
    "CIFAR10 이미지 분류기를 훈련하기 위해 [파이토치 문서에서 이 튜토리얼을](https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html) 확장할 것입니다.\n",
    "\n",
    "아래와 같이 약간의 수정만 추가하면 됩니다.\n",
    "\n",
    "1. 함수에서 데이터 로딩 및 학습 부분을 감싸두고,\n",
    "2. 일부 네트워크 파라미터를 구성 가능하게 하고,\n",
    "3. 체크포인트를 추가하고 (선택 사항),\n",
    "4. 모델 튜닝을 위한 검색 공간을 정의합니다.\n",
    "\n",
    "\n",
    "이 튜토리얼을 실행하기 위해 아래의 패키지가 설치되어 있는지 확인하세요:\n",
    "\n",
    "-  ``ray[tune]``: 배포된 하이퍼파라미터 튜닝 라이브러리\n",
    "-  ``torchvision``: 데이터 변형을 위해 필요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ix1PDKbM8llE"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from ray import tune, train\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6XECxCZ8llE"
   },
   "source": [
    "## Data loaders\n",
    "data loader를 자체 함수로 감싸두고 전역 데이터 디렉토리로 전달합니다.\n",
    "이런 식으로 서로 다른 실험들 간에 데이터 디렉토리를 공유할 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3J1A0NlS8llE"
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    # 데이터 전처리를 위한 변환(transform)을 정의\n",
    "    # ToTensor를 사용하여 이미지 데이터를 PyTorch 텐서로 변환하고,\n",
    "    # Normalize를 사용하여 이미지의 각 채널을 정규화.\n",
    "    # 정규화는 평균이 0.5, 표준편차가 0.5가 되도록 설정.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # CIFAR-10 훈련 데이터셋을 다운로드하고, 위에서 정의한 변환을 적용.\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M7iXrQd8llE"
   },
   "source": [
    "## 구성 가능한 신경망\n",
    "구성 가능한 파라미터만 튜닝이 가능합니다.\n",
    "이 예시를 통해 fully connected layer 크기를 지정할 수 있습니다:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rIMLTR0r8llE"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhPhoHQL8llF"
   },
   "source": [
    "## 학습 함수\n",
    "학습 스크립트를 ``train_cifar(config, checkpoint_dir=None, data_dir=None)`` 함수로 감싸둡니다.\n",
    "``config`` 매개변수는 훈련할 하이퍼파라미터를 받습니다. ``checkpoint_dir`` 매개변수는 체크포인트를 복원하는 데 사용됩니다. ``data_dir`` 은 데이터를 읽고 저장하는 디렉토리를 지정하므로,\n",
    "여러 실행들이 동일한 데이터 소스를 공유할 수 있습니다.\n",
    "\n",
    "```python\n",
    "net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "if checkpoint_dir:\n",
    "    model_state, optimizer_state = torch.load(\n",
    "        os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "    net.load_state_dict(model_state)\n",
    "    optimizer.load_state_dict(optimizer_state)\n",
    "```\n",
    "또한, 옵티마이저의 학습률(learning rate)을 구성할 수 있습니다.\n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "```\n",
    "학습 데이터를 학습 및 검증 세트로 나눕니다. 따라서 데이터의 80%는 모델 학습에 사용하고, 나머지 20%에 대해 유효성 검사 및 손실을 계산합니다. 학습 및 테스트 세트를 반복하는 배치 크기도 구성할 수 있습니다.\n",
    "\n",
    "### DataParallel을 이용한 GPU(다중)지원 추가\n",
    "이미지 분류는 GPU를 사용할 때 이점이 많습니다. 운좋게도 Ray Tune에서 파이토치의 추상화를 계속 사용할 수 있습니다.\n",
    "따라서 여러 GPU에서 데이터 병렬 훈련을 지원하기 위해 모델을 ``nn.DataParallel`` 으로 감쌀 수 있습니다.\n",
    "\n",
    "```python\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "```\n",
    "``device`` 변수를 사용하여 사용 가능한 GPU가 없을 때도 학습이 가능한지 확인합니다.\n",
    "\n",
    "```python\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "```\n",
    "이 코드는 CPU들, 단일 GPU 및 다중 GPU에 대한 학습을 지원합니다.\n",
    "특히 Ray는 [fractional-GPU](https://docs.ray.io/en/master/using-ray-with-gpus.html#fractional-gpus) 도 지원하므로\n",
    "모델이 GPU 메모리에 적합한 상황에서는 테스트 간에 GPU를 공유할 수 있습니다.\n",
    "\n",
    "### Ray Tune과 소통하기\n",
    "\n",
    "가장 흥미로운 부분은 Ray Tune과의 소통입니다.\n",
    "\n",
    "```python\n",
    "with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "    path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "    torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "```\n",
    "여기서 먼저 체크포인트를 저장한 다음 일부 메트릭을 Ray Tune에 다시 보냅니다. 특히, validation loss와 accuracy를\n",
    "Ray Tune으로 다시 보냅니다. 그 후 Ray Tune은 이러한 메트릭을 사용하여 최상의 결과를 유도하는 하이퍼파라미터 구성을\n",
    "결정할 수 있습니다. 이러한 메트릭들은 또한 리소스 낭비를 방지하기 위해 성능이 좋지 않은 실험을 조기에 중지하는 데 사용할 수 있습니다.\n",
    "\n",
    "체크포인트 저장은 선택사항이지만 [Population Based Training](https://docs.ray.io/en/master/tune/tutorials/tune-advanced-tutorial.html)\n",
    "과 같은 고급 스케줄러를 사용하려면 필요합니다. 또한 체크포인트를 저장하면 나중에 학습된 모델을 로드하고 평가 세트(test set)에서 검증할 수 있습니다.\n",
    "\n",
    "### Full training function\n",
    "\n",
    "전체 예제 코드는 다음과 같습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_69ubm8fNd-0"
   },
   "outputs": [],
   "source": [
    "def train_cifar(config, data_dir=None):\n",
    "    # Net 모델 인스턴스를 생성합니다. l1과 l2는 구성에서 지정된 레이어 크기입니다.\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    # 디바이스 설정: CUDA가 사용 가능한 경우 GPU를 사용하고, 그렇지 않으면 CPU를 사용합니다.\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        # 사용 가능한 CUDA 장치가 여러 개인 경우, DataParallel을 사용하여 모델을 병렬로 실행합니다.\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    # 손실 함수와 옵티마이저를 정의합니다.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    # 체크포인트가 있는 경우, 체크포인트에서 모델 상태와 옵티마이저 상태를 로드합니다.\n",
    "    checkpoint = train.get_checkpoint()\n",
    "    if checkpoint is not None:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "            net.load_state_dict(model_state)\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    # 학습 데이터셋을 로드하고, 학습 세트와 검증 세트로 분할합니다.\n",
    "    trainset, _ = load_data(data_dir)\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    # DataLoader를 사용하여 학습 및 검증 데이터를 배치로 로드합니다.\n",
    "    trainloader = DataLoader(train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=2)\n",
    "    valloader = DataLoader(val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=2)\n",
    "\n",
    "    # 학습 루프\n",
    "    for epoch in range(10):\n",
    "        net.train()\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 검증 루프\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # 검증 단계 후의 메트릭을 계산하고 ray.train.report를 사용하여 보고합니다.\n",
    "        metrics = {\"loss\": val_loss / total, \"accuracy\": correct / total}\n",
    "        train.report(metrics)\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kelxtDH8llF"
   },
   "source": [
    "## Test set 정확도(accuracy)\n",
    "일반적으로 머신러닝 모델의 성능은 모델 학습에 사용되지 않은 데이터를 사용해 테스트합니다.\n",
    "Test set 또한 함수로 감싸둘 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3_42CchG8llF"
   },
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    # 학습 데이터셋과 테스트 데이터셋을 로드합니다. 여기서는 테스트 세트만 사용됩니다.\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    # 테스트 데이터셋을 DataLoader를 통해 로드합니다.\n",
    "    # 배치 크기는 4로 설정하고, 데이터를 섞지 않으며, 병렬 처리를 위해 2개의 워커를 사용합니다.\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    # 올바르게 예측된 샘플 수와 전체 샘플 수를 추적하는 변수를 초기화합니다.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 기울기를 계산할 필요가 없으므로, no_grad() 컨텍스트를 사용하여 메모리 사용량을 줄이고 연산 속도를 높입니다.\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # 데이터와 레이블을 설정된 디바이스(CPU 또는 CUDA)로 이동합니다.\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # 현재 배치의 이미지에 대해 모델의 출력(예측)을 계산합니다.\n",
    "            outputs = net(images)\n",
    "            # 출력에서 가장 높은 값(가장 확신하는 클래스)을 가진 인덱스를 찾습니다.\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # 전체 샘플 수에 이 배치의 샘플 수를 더합니다.\n",
    "            total += labels.size(0)\n",
    "            # 올바르게 예측된 샘플의 수를 더합니다. 예측과 실제 레이블이 일치하는 경우의 수입니다.\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # 전체 샘플 대비 올바르게 예측된 샘플의 비율(정확도)를 계산하여 반환합니다.\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jf19Pe2Q8llF"
   },
   "source": [
    "이 함수는 또한 ``device`` 파라미터를 요구하므로, test set 평가를 GPU에서 수행할 수 있습니다.\n",
    "\n",
    "## 검색 공간 구성\n",
    "마지막으로 Ray Tune의 검색 공간을 정의해야 합니다. 예시는 아래와 같습니다.\n",
    "\n",
    "```python\n",
    "config = {\n",
    "    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "}\n",
    "```\n",
    "``tune.sample_from()`` 함수를 사용하면 고유한 샘플 방법을 정의하여 하이퍼파라미터를 얻을 수 있습니다.\n",
    "이 예제에서 ``l1`` 과 ``l2`` 파라미터는 4와 256 사이의 2의 거듭제곱이어야 하므로 4, 8, 16, 32, 64, 128, 256입니다.\n",
    "``lr`` (학습률)은 0.0001과 0.1 사이에서 균일하게 샘플링 되어야 합니다. 마지막으로, 배치 크기는 2, 4, 8, 16중에서 선택할 수 있습니다.\n",
    "\n",
    "각 실험에서, Ray Tune은 이제 이러한 검색 공간에서 매개변수 조합을 무작위로 샘플링합니다.\n",
    "그런 다음 여러 모델을 병렬로 훈련하고 이 중에서 가장 성능이 좋은 모델을 찾습니다. 또한 성능이 좋지 않은 실험을 조기에 종료하는 ``ASHAScheduler`` 를 사용합니다.\n",
    "\n",
    "상수 ``data_dir`` 파라미터를 설정하기 위해 ``functools.partial`` 로 ``train_cifar`` 함수를 감싸둡니다. 또한 각 실험에 사용할 수 있는 자원들(resources)을 Ray Tune에 알릴 수 있습니다.\n",
    "\n",
    "```python\n",
    "gpus_per_trial = 2\n",
    "# ...\n",
    "result = tune.run(\n",
    "    partial(train_cifar, data_dir=data_dir),\n",
    "    resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    checkpoint_at_end=True)\n",
    "```\n",
    "파이토치 ``DataLoader`` 인스턴스의 ``num_workers`` 을 늘리기 위해 CPU 수를 지정하고 사용할 수 있습니다.\n",
    "각 실험에서 선택한 수의 GPU들은 파이토치에 표시됩니다. 실험들은 요청되지 않은 GPU에 액세스할 수 없으므로 같은 자원들을 사용하는 중복된 실험에 대해 신경쓰지 않아도 됩니다.\n",
    "\n",
    "부분 GPUs를 지정할 수도 있으므로, ``gpus_per_trial=0.5`` 와 같은 것 또한 가능합니다. 이후 각 실험은 GPU를 공유합니다. 사용자는 모델이 여전히 GPU메모리에 적합한지만 확인하면 됩니다.\n",
    "\n",
    "모델을 훈련시킨 후, 가장 성능이 좋은 모델을 찾고 체크포인트 파일에서 학습된 모델을 로드합니다. 이후 test set 정확도(accuracy)를 얻고 모든 것들을 출력하여 확인할 수 있습니다.\n",
    "\n",
    "전체 주요 기능은 다음과 같습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9n7YIu08llG",
    "outputId": "3669e1af-ef3a-4fad-95d7-091c64ac025c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:04<00:00, 34626749.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /content/data/cifar-10-python.tar.gz to /content/data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 00:44:42,393\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-05-22 00:44:46,057\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-05-22 00:44:46,893\tWARNING tune.py:900 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "| Configuration for experiment     train_cifar_2024-05-22_00-44-46   |\n",
      "+--------------------------------------------------------------------+\n",
      "| Search algorithm                 BasicVariantGenerator             |\n",
      "| Scheduler                        AsyncHyperBandScheduler           |\n",
      "| Number of trials                 10                                |\n",
      "+--------------------------------------------------------------------+\n",
      "\n",
      "View detailed results here: /root/ray_results/train_cifar_2024-05-22_00-44-46\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-05-22_00-44-36_381171_1581/artifacts/2024-05-22_00-44-46/train_cifar_2024-05-22_00-44-46/driver_artifacts`\n",
      "\n",
      "Trial status: 10 PENDING\n",
      "Current time: 2024-05-22 00:44:47. Total running time: 0s\n",
      "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size |\n",
      "+-----------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   PENDING    0.0331583                2 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4 |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2 |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8 |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8 |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16 |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8 |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4 |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16 |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2 |\n",
      "+-----------------------------------------------------------------+\n",
      "\n",
      "Trial train_cifar_7c8c5_00000 started with configuration:\n",
      "+--------------------------------------------------+\n",
      "| Trial train_cifar_7c8c5_00000 config             |\n",
      "+--------------------------------------------------+\n",
      "| batch_size                                     2 |\n",
      "| l1                                            64 |\n",
      "| l2                                           128 |\n",
      "| lr                                       0.03316 |\n",
      "+--------------------------------------------------+\n",
      "\u001b[36m(func pid=2277)\u001b[0m Files already downloaded and verified\n",
      "\u001b[36m(func pid=2277)\u001b[0m Files already downloaded and verified\n",
      "\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:45:17. Total running time: 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size |\n",
      "+-----------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4 |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2 |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8 |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8 |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16 |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8 |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4 |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16 |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2 |\n",
      "+-----------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:45:47. Total running time: 1min 0s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size |\n",
      "+-----------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4 |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2 |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8 |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8 |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16 |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8 |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4 |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16 |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2 |\n",
      "+-----------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:46:17. Total running time: 1min 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size |\n",
      "+-----------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4 |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2 |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8 |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8 |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16 |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8 |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4 |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16 |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2 |\n",
      "+-----------------------------------------------------------------+\n",
      "\n",
      "Trial train_cifar_7c8c5_00000 finished iteration 1 at 2024-05-22 00:46:27. Total running time: 1min 40s\n",
      "+--------------------------------------------------+\n",
      "| Trial train_cifar_7c8c5_00000 result             |\n",
      "+--------------------------------------------------+\n",
      "| checkpoint_dir_name                              |\n",
      "| time_this_iter_s                         95.9056 |\n",
      "| time_total_s                             95.9056 |\n",
      "| training_iteration                             1 |\n",
      "| accuracy                                  0.0983 |\n",
      "| loss                                     1.16071 |\n",
      "+--------------------------------------------------+\n",
      "\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:46:47. Total running time: 2min 0s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        1            95.9056   1.16071       0.0983 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:47:17. Total running time: 2min 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        1            95.9056   1.16071       0.0983 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:47:47. Total running time: 3min 0s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        1            95.9056   1.16071       0.0983 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_cifar_7c8c5_00000 finished iteration 2 at 2024-05-22 00:48:00. Total running time: 3min 13s\n",
      "+--------------------------------------------------+\n",
      "| Trial train_cifar_7c8c5_00000 result             |\n",
      "+--------------------------------------------------+\n",
      "| checkpoint_dir_name                              |\n",
      "| time_this_iter_s                         92.7245 |\n",
      "| time_total_s                              188.63 |\n",
      "| training_iteration                             2 |\n",
      "| accuracy                                  0.0977 |\n",
      "| loss                                     1.17381 |\n",
      "+--------------------------------------------------+\n",
      "\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:48:17. Total running time: 3min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        2             188.63   1.17381       0.0977 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:48:48. Total running time: 4min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        2             188.63   1.17381       0.0977 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:49:18. Total running time: 4min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        2             188.63   1.17381       0.0977 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_cifar_7c8c5_00000 finished iteration 3 at 2024-05-22 00:49:32. Total running time: 4min 45s\n",
      "+--------------------------------------------------+\n",
      "| Trial train_cifar_7c8c5_00000 result             |\n",
      "+--------------------------------------------------+\n",
      "| checkpoint_dir_name                              |\n",
      "| time_this_iter_s                          92.518 |\n",
      "| time_total_s                             281.148 |\n",
      "| training_iteration                             3 |\n",
      "| accuracy                                  0.1043 |\n",
      "| loss                                      1.1612 |\n",
      "+--------------------------------------------------+\n",
      "\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:49:48. Total running time: 5min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)     loss     accuracy |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        3            281.148   1.1612       0.1043 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                   |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                   |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                   |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                   |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                   |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                   |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                   |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                   |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:50:18. Total running time: 5min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)     loss     accuracy |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        3            281.148   1.1612       0.1043 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                   |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                   |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                   |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                   |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                   |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                   |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                   |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                   |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:50:48. Total running time: 6min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)     loss     accuracy |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        3            281.148   1.1612       0.1043 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                   |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                   |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                   |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                   |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                   |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                   |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                   |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                   |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_cifar_7c8c5_00000 finished iteration 4 at 2024-05-22 00:51:06. Total running time: 6min 19s\n",
      "+--------------------------------------------------+\n",
      "| Trial train_cifar_7c8c5_00000 result             |\n",
      "+--------------------------------------------------+\n",
      "| checkpoint_dir_name                              |\n",
      "| time_this_iter_s                         93.2626 |\n",
      "| time_total_s                             374.411 |\n",
      "| training_iteration                             4 |\n",
      "| accuracy                                  0.1002 |\n",
      "| loss                                     1.16082 |\n",
      "+--------------------------------------------------+\n",
      "\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:51:18. Total running time: 6min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        4            374.411   1.16082       0.1002 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:51:48. Total running time: 7min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        4            374.411   1.16082       0.1002 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:52:18. Total running time: 7min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        4            374.411   1.16082       0.1002 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_cifar_7c8c5_00000 finished iteration 5 at 2024-05-22 00:52:39. Total running time: 7min 52s\n",
      "+--------------------------------------------------+\n",
      "| Trial train_cifar_7c8c5_00000 result             |\n",
      "+--------------------------------------------------+\n",
      "| checkpoint_dir_name                              |\n",
      "| time_this_iter_s                         93.1043 |\n",
      "| time_total_s                             467.515 |\n",
      "| training_iteration                             5 |\n",
      "| accuracy                                  0.0977 |\n",
      "| loss                                     1.18929 |\n",
      "+--------------------------------------------------+\n",
      "\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:52:48. Total running time: 8min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        5            467.515   1.18929       0.0977 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:53:18. Total running time: 8min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        5            467.515   1.18929       0.0977 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:53:48. Total running time: 9min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        5            467.515   1.18929       0.0977 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_cifar_7c8c5_00000 finished iteration 6 at 2024-05-22 00:54:11. Total running time: 9min 24s\n",
      "+--------------------------------------------------+\n",
      "| Trial train_cifar_7c8c5_00000 result             |\n",
      "+--------------------------------------------------+\n",
      "| checkpoint_dir_name                              |\n",
      "| time_this_iter_s                         92.1105 |\n",
      "| time_total_s                             559.625 |\n",
      "| training_iteration                             6 |\n",
      "| accuracy                                  0.1009 |\n",
      "| loss                                     1.19151 |\n",
      "+--------------------------------------------------+\n",
      "\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:54:18. Total running time: 9min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        6            559.625   1.19151       0.1009 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:54:48. Total running time: 10min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        6            559.625   1.19151       0.1009 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:55:18. Total running time: 10min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        6            559.625   1.19151       0.1009 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_cifar_7c8c5_00000 finished iteration 7 at 2024-05-22 00:55:44. Total running time: 10min 57s\n",
      "+--------------------------------------------------+\n",
      "| Trial train_cifar_7c8c5_00000 result             |\n",
      "+--------------------------------------------------+\n",
      "| checkpoint_dir_name                              |\n",
      "| time_this_iter_s                         93.4556 |\n",
      "| time_total_s                             653.081 |\n",
      "| training_iteration                             7 |\n",
      "| accuracy                                  0.0996 |\n",
      "| loss                                     1.15761 |\n",
      "+--------------------------------------------------+\n",
      "\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:55:48. Total running time: 11min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        7            653.081   1.15761       0.0996 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:56:18. Total running time: 11min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        7            653.081   1.15761       0.0996 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 00:56:25,046\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-05-22 00:56:25,056\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-22 00:56:25,063\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_cifar_2024-05-22_00-44-46' in 0.0104s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial status: 1 RUNNING | 9 PENDING\n",
      "Current time: 2024-05-22 00:56:25. Total running time: 11min 38s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                status              lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "| train_cifar_7c8c5_00000   RUNNING    0.0331583                2        7            653.081   1.15761       0.0996 |\n",
      "| train_cifar_7c8c5_00001   PENDING    0.00322777               4                                                    |\n",
      "| train_cifar_7c8c5_00002   PENDING    0.000446491              2                                                    |\n",
      "| train_cifar_7c8c5_00003   PENDING    0.000242495              8                                                    |\n",
      "| train_cifar_7c8c5_00004   PENDING    0.000326941              8                                                    |\n",
      "| train_cifar_7c8c5_00005   PENDING    0.00331424              16                                                    |\n",
      "| train_cifar_7c8c5_00006   PENDING    0.00164078               8                                                    |\n",
      "| train_cifar_7c8c5_00007   PENDING    0.00226424               4                                                    |\n",
      "| train_cifar_7c8c5_00008   PENDING    0.000499777             16                                                    |\n",
      "| train_cifar_7c8c5_00009   PENDING    0.00425958               2                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 00:56:35,099\tWARNING tune.py:1054 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2024-05-22 00:56:35,117\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 9 trial(s):\n",
      "- train_cifar_7c8c5_00001: FileNotFoundError('Could not fetch metrics for train_cifar_7c8c5_00001: both result.json and progress.csv were not found at /root/ray_results/train_cifar_2024-05-22_00-44-46/train_cifar_7c8c5_00001_1_batch_size=4,lr=0.0032_2024-05-22_00-44-47')\n",
      "- train_cifar_7c8c5_00002: FileNotFoundError('Could not fetch metrics for train_cifar_7c8c5_00002: both result.json and progress.csv were not found at /root/ray_results/train_cifar_2024-05-22_00-44-46/train_cifar_7c8c5_00002_2_batch_size=2,lr=0.0004_2024-05-22_00-44-47')\n",
      "- train_cifar_7c8c5_00003: FileNotFoundError('Could not fetch metrics for train_cifar_7c8c5_00003: both result.json and progress.csv were not found at /root/ray_results/train_cifar_2024-05-22_00-44-46/train_cifar_7c8c5_00003_3_batch_size=8,lr=0.0002_2024-05-22_00-44-47')\n",
      "- train_cifar_7c8c5_00004: FileNotFoundError('Could not fetch metrics for train_cifar_7c8c5_00004: both result.json and progress.csv were not found at /root/ray_results/train_cifar_2024-05-22_00-44-46/train_cifar_7c8c5_00004_4_batch_size=8,lr=0.0003_2024-05-22_00-44-47')\n",
      "- train_cifar_7c8c5_00005: FileNotFoundError('Could not fetch metrics for train_cifar_7c8c5_00005: both result.json and progress.csv were not found at /root/ray_results/train_cifar_2024-05-22_00-44-46/train_cifar_7c8c5_00005_5_batch_size=16,lr=0.0033_2024-05-22_00-44-47')\n",
      "- train_cifar_7c8c5_00006: FileNotFoundError('Could not fetch metrics for train_cifar_7c8c5_00006: both result.json and progress.csv were not found at /root/ray_results/train_cifar_2024-05-22_00-44-46/train_cifar_7c8c5_00006_6_batch_size=8,lr=0.0016_2024-05-22_00-44-47')\n",
      "- train_cifar_7c8c5_00007: FileNotFoundError('Could not fetch metrics for train_cifar_7c8c5_00007: both result.json and progress.csv were not found at /root/ray_results/train_cifar_2024-05-22_00-44-46/train_cifar_7c8c5_00007_7_batch_size=4,lr=0.0023_2024-05-22_00-44-47')\n",
      "- train_cifar_7c8c5_00008: FileNotFoundError('Could not fetch metrics for train_cifar_7c8c5_00008: both result.json and progress.csv were not found at /root/ray_results/train_cifar_2024-05-22_00-44-46/train_cifar_7c8c5_00008_8_batch_size=16,lr=0.0005_2024-05-22_00-44-47')\n",
      "- train_cifar_7c8c5_00009: FileNotFoundError('Could not fetch metrics for train_cifar_7c8c5_00009: both result.json and progress.csv were not found at /root/ray_results/train_cifar_2024-05-22_00-44-46/train_cifar_7c8c5_00009_9_batch_size=2,lr=0.0043_2024-05-22_00-44-47')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial config: {'l1': 64, 'l2': 128, 'lr': 0.03315830989668164, 'batch_size': 2}\n",
      "Best trial final validation loss: 1.1576066904306412\n",
      "Best trial final validation accuracy: 0.0996\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Best trial test set accuracy: 0.0994\n"
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    # 데이터셋이 위치할 디렉토리의 절대 경로를 설정합니다.\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    # 데이터셋을 로드합니다.\n",
    "    load_data(data_dir)\n",
    "\n",
    "    # 하이퍼파라미터 설정을 위한 구성입니다.\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),  # 첫 번째 레이어의 유닛 수\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),  # 두 번째 레이어의 유닛 수\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),  # 학습률\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])  # 배치 크기\n",
    "    }\n",
    "\n",
    "    # ASHAScheduler를 사용하여 조기 중단을 구성합니다.\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",  # 최적화할 메트릭\n",
    "        mode=\"min\",  # 메트릭을 최소화\n",
    "        max_t=max_num_epochs,  # 최대 에폭 수\n",
    "        grace_period=1,  # 조기 중단 전 최소 에폭 수\n",
    "        reduction_factor=2)  # 트라이얼 감소 비율\n",
    "\n",
    "    # CLIReporter를 사용하여 트라이얼 진행 상황을 터미널에 보고합니다.\n",
    "    reporter = CLIReporter(metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "\n",
    "    # tune.run을 사용하여 학습을 실행합니다.\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=data_dir),  # 학습 함수\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},  # 트라이얼당 리소스\n",
    "        config=config,\n",
    "        num_samples=num_samples,  # 샘플링할 구성의 수\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    # 가장 좋은 트라이얼을 선택합니다.\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    # 가장 좋은 트라이얼의 모델을 로드합니다.\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    # 체크포인트에서 모델 상태를 로드합니다.\n",
    "    if best_trial.checkpoint:\n",
    "        with best_trial.checkpoint.as_directory() as best_checkpoint_dir:\n",
    "            model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "            best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    # 테스트 데이터셋에서 모델의 정확도를 평가합니다.\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-aiALU58llG"
   },
   "source": [
    "코드를 실행하면 결과는 다음과 같습니다.\n",
    "\n",
    "::\n",
    "\n",
    "    Number of trials: 10 (10 TERMINATED)\n",
    "    +-----+------+------+-------------+--------------+---------+------------+--------------------+\n",
    "    | ... |   l1 |   l2 |          lr |   batch_size |    loss |   accuracy | training_iteration |\n",
    "    |-----+------+------+-------------+--------------+---------+------------+--------------------|\n",
    "    | ... |   64 |    4 | 0.00011629  |            2 | 1.87273 |     0.244  |                  2 |\n",
    "    | ... |   32 |   64 | 0.000339763 |            8 | 1.23603 |     0.567  |                  8 |\n",
    "    | ... |    8 |   16 | 0.00276249  |           16 | 1.1815  |     0.5836 |                 10 |\n",
    "    | ... |    4 |   64 | 0.000648721 |            4 | 1.31131 |     0.5224 |                  8 |\n",
    "    | ... |   32 |   16 | 0.000340753 |            8 | 1.26454 |     0.5444 |                  8 |\n",
    "    | ... |    8 |    4 | 0.000699775 |            8 | 1.99594 |     0.1983 |                  2 |\n",
    "    | ... |  256 |    8 | 0.0839654   |           16 | 2.3119  |     0.0993 |                  1 |\n",
    "    | ... |   16 |  128 | 0.0758154   |           16 | 2.33575 |     0.1327 |                  1 |\n",
    "    | ... |   16 |    8 | 0.0763312   |           16 | 2.31129 |     0.1042 |                  4 |\n",
    "    | ... |  128 |   16 | 0.000124903 |            4 | 2.26917 |     0.1945 |                  1 |\n",
    "    +-----+------+------+-------------+--------------+---------+------------+--------------------+\n",
    "\n",
    "\n",
    "    Best trial config: {'l1': 8, 'l2': 16, 'lr': 0.00276249, 'batch_size': 16, 'data_dir': '...'}\n",
    "    Best trial final validation loss: 1.181501\n",
    "    Best trial final validation accuracy: 0.5836\n",
    "    Best trial test set accuracy: 0.5806\n",
    "\n",
    "대부분의 실험은 자원 낭비를 막기 위해 일찍 중단되었습니다. 가장 좋은 결과를 얻은 실험은 58%의 정확도를 달성했으며, 이는 테스트 세트에서 확인할 수 있습니다.\n",
    "\n",
    "이제 파이토치 모델의 매개변수를 조정할 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p_qVMrrHFL_0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
