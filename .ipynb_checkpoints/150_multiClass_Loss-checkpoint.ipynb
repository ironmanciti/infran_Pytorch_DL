{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 150. PyTorch 다중 분류 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Crossentropy\n",
    "    \n",
    "<img src=\"https://gombru.github.io/assets/cross_entropy_loss/softmax_CE_pipeline.png\" width=500 />\n",
    "\n",
    "##  `nn.CrossEntropyLoss` \n",
    "\n",
    "- `nn.LogSoftmax()` 와 `nn.NLLLoss()` 를 단일 class 로 combine한 것.\n",
    "\n",
    "    즉, `nn.CrossEntropyLoss()` = `nn.NLLLoss(nn.LogSoftmax())`\n",
    "\n",
    "### nn.CrossEntropyLoss\n",
    "\n",
    "$$loss(x, class) = -\\log(\\frac{exp(x[class])}{\\sum_{j}{exp(x[j])}}) = -x[class] + log(\\sum_{j}exp(x[j]))$$\n",
    "\n",
    "### nn.LogSoftmax\n",
    "\n",
    "$$LogSoftmax(x_i) = \\log(\\frac{exp(x_i)}{\\sum_{j}{exp(x_j)}})$$\n",
    "\n",
    "### nn.NLLLoss (Negative Log Likelihood Loss)\n",
    "$$l(x, y) = - \\frac{1}{N} \\sum_{1}^{N}l_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론적으로, Pytorch 에서는 Cross Entropy Loss 값의 결과를 얻기 위해 2가지 방식이 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLLLoss(logs, targets):\n",
    "    out = torch.zeros_like(targets, dtype=torch.float)\n",
    "    for i in range(len(targets)):\n",
    "        out[i] = logs[i][targets[i]]\n",
    "    return -out.sum()/len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class label 이 1 인 다중분류 \n",
    "x = torch.Tensor([[0.8982, 0.805, 0.6393, 0.9983, 0.5731, 0.0469, 0.556, 0.1476, 0.8404, 0.5544]])\n",
    "y = torch.LongTensor([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법 1\n",
    "### nn.CrossEntropyLoss 함수 사용\n",
    "\n",
    "-  `NNLLoss(Log(Softmax))`\n",
    "\n",
    "$-x[class] + log(\\sum_{j}exp(x[j]))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1438)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "cross_entropy_loss(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법 2\n",
    "### nn.LogSoftmax + NLLLoss \n",
    "\n",
    "- LogSoftmax : Log + Softmax - 두 함수를 따로 적용한 것 보다 수학적 안정성이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1438)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "x_log = log_softmax(x)\n",
    "NLLLoss(x_log, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1438)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.NLLLoss()(x_log, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용상의 주의 사항 \n",
    "\n",
    "#### nn.CrossEntropyLoss 를 사용할 경우\n",
    "-  nn.CrossEntropyLoss 내에 softmax 함수가 포함되어 있으므로 Neural Network의 마지막 activation 함수로 Softmax를 지정 않고, logit 만 출력한다.  \n",
    "\n",
    "#### nn.NLLLoss 를 사용할 경우 \n",
    "- nn.NLLLoss 는 입력으로 확률 분포가 와야 하므로, Neural Network의 마지막 actiovation 함수로 LogSoftmax를 지정 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmax index\n",
      "tensor([2.0000, 3.1000, 1.0000]) tensor([0, 2, 1])\n",
      "tensor([2.5000, 3.1000, 3.1000]) tensor([2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "cross_entropyloss = nn.CrossEntropyLoss()\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "negative_LLLoss = nn.NLLLoss()\n",
    "\n",
    "# true label\n",
    "Y = torch.tensor([0, 2, 1])  \n",
    "\n",
    "# good prediction\n",
    "y_pred_good = torch.tensor([[2.0, 1.0, 0.1], [0.5, 1.0, 3.1], [0.3, 1.0, 0.1]])\n",
    "# bad prediction\n",
    "y_pred_bad  = torch.tensor([[0.1, 1.0, 2.5], [3.1, 1.0, 0.5], [3.1, 0.1, 2.5]])\n",
    "\n",
    "print(\"argmax index\")\n",
    "_, pred1 = torch.max(y_pred_good, 1)\n",
    "print(_, pred1)\n",
    "\n",
    "_, pred2 = torch.max(y_pred_bad, 1)\n",
    "print(_, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn.CrossEntropyLoss() 사용 결과 :\n",
      "0.41337862610816956 2.973893404006958\n",
      "\n",
      "nn.LogSoftmax() + nn.NLLLoss() 사용 결과 :\n",
      "0.41337862610816956 2.973893404006958\n"
     ]
    }
   ],
   "source": [
    "# CrossEntropy Loss\n",
    "loss_good = cross_entropyloss(y_pred_good, Y)\n",
    "loss_bad = cross_entropyloss(y_pred_bad, Y)\n",
    "print('nn.CrossEntropyLoss() 사용 결과 :')\n",
    "print(loss_good.item(), loss_bad.item())\n",
    "print()\n",
    "\n",
    "# NLLLoss\n",
    "m1 = log_softmax(y_pred_good)\n",
    "m2 = log_softmax(y_pred_bad)\n",
    "\n",
    "loss_good = negative_LLLoss(m1, Y)\n",
    "loss_bad = negative_LLLoss(m2, Y)\n",
    "print('nn.LogSoftmax() + nn.NLLLoss() 사용 결과 :')\n",
    "print(loss_good.item(), loss_bad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward method 의 return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ex_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(2, 8)\n",
    "        self.linear2 = nn.Linear(8, 3)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward1(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        # CrossEntropyLoss를 사용할 경우\n",
    "        # no log softmax at the end\n",
    "        return x\n",
    "    \n",
    "    def forward2(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        # NLLLoss 를 사용할 경우\n",
    "        # log softmax 를 output 에 사용\n",
    "        x = self.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20., 10.],\n",
       "        [10., 30.],\n",
       "        [10.,  1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Ex_NN()\n",
    "\n",
    "x = torch.FloatTensor([[20, 10], [10, 30], [10, 1]])\n",
    "y = torch.tensor([0, 2, 1])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CrossEntropyLoss 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3101, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "logits = model.forward1(x)\n",
    "print(loss(logits, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Negative Log Likelihood Loss 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3101, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.NLLLoss()\n",
    "prob = model.forward2(x)\n",
    "print(loss(prob, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3639e+00, -3.4139e-01, -3.3949e+00],\n",
      "        [-5.9470e+00, -2.6188e-03, -1.3211e+01],\n",
      "        [-8.9062e-01, -1.3551e+00, -1.1036e+00]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(prob, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-0.3414, -0.0026, -0.8906], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([1, 1, 0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(prob, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
