{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJLhXD_zlp1A"
   },
   "source": [
    "# 125. Torchtext 와 RNN을 이용한 Sentiment Analysis\n",
    "\n",
    "### Torchtext 가 제공하는 기능들은 다음과 같습니다.\n",
    "\n",
    "- 파일 로드하기(File Loading) : 다양한 포맷의 코퍼스를 로드합니다.\n",
    "- 토큰화(Tokenization) : 문장을 단어 단위로 분리해 줍니다.\n",
    "- 단어 집합(Vocab) : 단어 집합을 만듭니다.\n",
    "- 정수 인코딩(Integer encoding) : 전체 코퍼스의 단어들을 각각의 고유한 정수로 맵핑합니다.\n",
    "- 단어 벡터(Word Vector) : 단어 집합의 단어들에 고유한 임베딩 벡터를 만들어줍니다. 랜덤값으로 초기화한 값일 수도 있고, 사전 훈련된 임베딩 벡터들을 로드할 수도 있습니다.\n",
    "- 배치화(Batching) : 훈련 샘플들의 배치를 만들어줍니다. 이 과정에서 Padding 도 이루어집니다.\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "<img src=\"network_diagram.png\" width=30%>\n",
    "\n",
    "**먼저, 임베딩 레이어에 단어를 전달할 것입니다.** 수만 개의 단어를 원-핫 인코딩보다 더 효율적으로 표현하려면 임베딩 레이어가 필요합니다.\n",
    "\n",
    "**입력 단어가 임베딩 레이어에 전달된 후 새 임베딩이 LSTM 셀로 전달됩니다.** \n",
    "\n",
    "**마지막으로 LSTM 출력은 시그모이드 출력 레이어로 이동합니다.** \n",
    "\n",
    "**마지막 출력**을 제외하고 시그모이드 출력은 신경 쓰지 않습니다. 나머지는 무시할 수 있습니다. 마지막 time step의 출력과 학습 레이블 (pos 또는 neg)을 비교하여 손실을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcFje9eRlp1B"
   },
   "source": [
    "### 데이터 로드 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "Pwc2EBYB5lnX",
    "outputId": "e1f0722a-6451-4ed1-aa7b-22e70b8e88ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import urllib.request as request\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 400)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVwBeJ44lp1H"
   },
   "source": [
    "## Data pre-processing\n",
    "\n",
    "\n",
    "신경망 모델을 구축 할 때 첫 번째 단계는 데이터를 적절한 형태로 가져와 네트워크에 공급하는 것입니다. 임베딩 레이어를 사용하기 때문에 각 단어를 정수로 인코딩해야합니다. \n",
    "위 데이터의 경우, 처리 단계는 다음과 같습니다.\n",
    ">* 구두점과 줄바꿈 문자 `\\n`을 없앱니다.  \n",
    "그런 다음 모든 텍스트를 개별 단어로 나눕니다 (tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "xFnknTrjlp1K",
    "outputId": "f6571e70-f033-4405-d55f-519974701ffe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./sample_data/IMBD_review.csv', <http.client.HTTPMessage at 0x13eb7378b48>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request.urlretrieve(\n",
    "    'https://raw.github.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv',\n",
    "    filename='./sample_data/IMBD_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local movies for the simple reason that they are poorly made, they lack the depth, and just not worth our time.&lt;br /&gt;&lt;br /&gt;The trailer of \"Nasaan ka man\" caught my attention, my daughter in law's and daughter's so we took time out to watch it this afternoon. The movie exceeded our expectations. The cinematography was very good, the story beautiful and the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the worst movie I had ever seen. Since that time, I have seen many more movies that are worse (how is it possible??) Therefore, to be fair, I had to give this movie a 2 out of 10. But it was a tough call.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                            review  \\\n",
       "0  My family and I normally do not watch local movies for the simple reason that they are poorly made, they lack the depth, and just not worth our time.<br /><br />The trailer of \"Nasaan ka man\" caught my attention, my daughter in law's and daughter's so we took time out to watch it this afternoon. The movie exceeded our expectations. The cinematography was very good, the story beautiful and the ...   \n",
       "1                                                                                                                                                             Believe it or not, this was at one time the worst movie I had ever seen. Since that time, I have seen many more movies that are worse (how is it possible??) Therefore, to be fair, I had to give this movie a 2 out of 10. But it was a tough call.   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/IMBD_review.csv')\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dNdrArLdlp1M",
    "outputId": "c0e66c64-588e-4f09-cd94-e5cbf63f421a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "7rHDKV0Nlp1O",
    "outputId": "e9f4b47d-7290-4864-946f-fadd732b1b13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my family and i normally do not watch local movies for the simple reason that they are poorly made they lack the depth and just not worth our time  the trailer of nasaan ka man caught my attention my daughter in laws and daughters so we took time out to watch it this afternoon the movie exceeded our expectations the cinematography was very good the story beautiful and the acting awesome jerich...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>believe it or not this was at one time the worst movie i had ever seen since that time i have seen many more movies that are worse how is it possible therefore to be fair i had to give this movie a 2 out of 10 but it was a tough call</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                            review  \\\n",
       "0  my family and i normally do not watch local movies for the simple reason that they are poorly made they lack the depth and just not worth our time  the trailer of nasaan ka man caught my attention my daughter in laws and daughters so we took time out to watch it this afternoon the movie exceeded our expectations the cinematography was very good the story beautiful and the acting awesome jerich...   \n",
       "1                                                                                                                                                                        believe it or not this was at one time the worst movie i had ever seen since that time i have seen many more movies that are worse how is it possible therefore to be fair i had to give this movie a 2 out of 10 but it was a tough call   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "    s = s.lower()\n",
    "    s = s.replace('<br', '')\n",
    "    s = s.replace('/>', '')\n",
    "    s = s.replace('\\n', '')\n",
    "    s = ''.join(c for c in s if c not in punctuation)\n",
    "    return s\n",
    "\n",
    "df['review'] = df['review'].apply(preprocess)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['review'].values\n",
    "y= df['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmV9DO4wlp1Q"
   },
   "source": [
    "### word dictionary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "\n",
    "for  line in X_train:\n",
    "    counter.update(tokenizer(line))\n",
    "\n",
    "vocab = Vocab(counter, max_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence의 길이 시각화 및 MAX_LENGTH 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2gfVvJglp1T"
   },
   "source": [
    "### Encoding the text\n",
    "\n",
    "- word dictionary 를 이용하여 all_reviews 를 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([len(x) for x in X_train], bins=50)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 200\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    tokenized = [vocab[token] for token in tokenizer(x)]\n",
    "    if len(tokenized ) < MAX_LENGTH:\n",
    "        X = [1] * (MAX_LENGTH - len(tokenized)) + tokenized \n",
    "    else:\n",
    "        X = tokenized [:MAX_LENGTH]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDB(Dataset):\n",
    "    def __init__(self, X, y, max_length, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.maxlen = max_length\n",
    "        self.len = len(X)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            X= self.transform(self.X[index])\n",
    "            \n",
    "        y = self.y[index]\n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "2Ikd8__Ilp1V",
    "outputId": "1ccf025d-64fb-45d0-f7ef-c455f794edec"
   },
   "outputs": [],
   "source": [
    "train_ds = IMDB(X_train, y_train, MAX_LENGTH, transform=text_pipeline)\n",
    "test_ds   = IMDB(X_test, y_test, MAX_LENGTH, transform=text_pipeline)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[   1,    1,    1,  ...,  123,    6,   81],\n",
       "         [   1,    1,    1,  ...,    8,  142,  469],\n",
       "         [   4,  207,   16,  ..., 1006,    0,   36],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    8,  144,    9],\n",
       "         [   0, 2312,  398,  ...,   99,   35,   65],\n",
       "         [   0, 1063,    7,  ..., 3530,  259, 1692]]),\n",
       " tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zH2A9mbelp1Z",
    "outputId": "40c89b81-9e7d-41e4-b7b3-10b6a20a14ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  5002\n"
     ]
    }
   ],
   "source": [
    "print('Unique words: ', len(vocab))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFRp4ah-lp1t"
   },
   "source": [
    "# Sentiment Network with PyTorch\n",
    "\n",
    "레이어는 다음과 같이 정의합니다.\n",
    "\n",
    "1. 단어 토큰(정수)을 특정 크기의 임베딩으로 변환하는 [임베딩 레이어](https://pytorch.org/docs/stable/nn.html#embedding).\n",
    "2. hidden_state 크기와 레이어 수로 정의 된 [LSTM 레이어](https://pytorch.org/docs/stable/nn.html#lstm)\n",
    "3. LSTM 계층 출력을 원하는 output_size에 매핑하는 fully-connected output layer\n",
    "4. 모든 출력을 0-1 값으로 바꾸는 sigmoid activation layer; 이 네트워크의 출력으로는 **마지막 sigmoid 출력만**을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PNMW4McVlp1v"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, \n",
    "                 n_layers, drop_prob=0.5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        hidden = (h0, c0)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        #out:(seq, batch, hidden)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0)) \n",
    "    \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        #out:(batch,outdim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKlcz0Atlp1x"
   },
   "source": [
    "## Instantiate the network\n",
    "\n",
    "하이퍼 파라미터를 정의합니다.\n",
    "\n",
    "* `vocab_size` : 어휘의 크기 또는 입력, 단어 토큰 값의 범위.\n",
    "* `output_size` : 원하는 출력의 크기; 출력하려는 클래스 score 의 갯수 (pos / neg).\n",
    "* `embedding_dim` : 임베딩 룩업 테이블의 열 수; 임베딩의 크기.\n",
    "* `hidden_dim` : LSTM 셀의 은닉층 unit 수. 일반적으로 클수록 성능이 더 좋습니다. 일반적인 값은 128, 256, 512 등입니다.\n",
    "* `n_layers` : 네트워크의 LSTM 계층 수. 일반적으로 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "goj3maV6lp1x",
    "outputId": "1272051f-b745-4a32-fab0-28abd95e0db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(5002, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) \n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, \n",
    "                   hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pubcvw2Ylp1z"
   },
   "source": [
    "## Training\n",
    "\n",
    "> 우리는 단일 Sigmoid 출력으로 작동하도록 설계된 새로운 종류의 교차 엔트로피 손실을 사용할 것입니다. [BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss)   \n",
    "**Binary Cross Entropy Loss**는 교차 엔트로피 손실을 0과 1 사이의 단일 값에 적용합니다.\n",
    "\n",
    "- BCEWithLogitsLoss : Sigmoid + BCELoss\n",
    "\n",
    "- training hyparameter\n",
    "\n",
    "* `lr` : learning rate\n",
    "* `epochs` : 훈련 데이터 세트를 반복하는 횟수.\n",
    "*` clip` : 클리핑 할 최대 그라디언트 값 (그라디언트 exploding 방지)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oxaSarGQlp1z"
   },
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "LEBL3tcFlp11",
    "outputId": "0416cea1-d59a-4ba0-fcb6-28c6156d9d31",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.686827... Val Loss: 0.682771\n",
      "Epoch: 1/4... Step: 200... Loss: 0.654057... Val Loss: 0.639731\n",
      "Epoch: 1/4... Step: 300... Loss: 0.655761... Val Loss: 0.551586\n",
      "Epoch: 1/4... Step: 400... Loss: 0.665268... Val Loss: 0.564048\n",
      "Epoch: 1/4... Step: 500... Loss: 0.531316... Val Loss: 0.540554\n",
      "Epoch: 1/4... Step: 600... Loss: 0.493906... Val Loss: 0.561348\n",
      "Epoch: 1/4... Step: 700... Loss: 0.388061... Val Loss: 0.484828\n",
      "Epoch: 2/4... Step: 800... Loss: 0.419710... Val Loss: 0.468360\n",
      "Epoch: 2/4... Step: 900... Loss: 0.473528... Val Loss: 0.421902\n",
      "Epoch: 2/4... Step: 1000... Loss: 0.292245... Val Loss: 0.471610\n",
      "Epoch: 2/4... Step: 1100... Loss: 0.360984... Val Loss: 0.399889\n",
      "Epoch: 2/4... Step: 1200... Loss: 0.405506... Val Loss: 0.415911\n",
      "Epoch: 2/4... Step: 1300... Loss: 0.532954... Val Loss: 0.425474\n",
      "Epoch: 2/4... Step: 1400... Loss: 0.339821... Val Loss: 0.386594\n",
      "Epoch: 2/4... Step: 1500... Loss: 0.396870... Val Loss: 0.372937\n",
      "Epoch: 3/4... Step: 1600... Loss: 0.461203... Val Loss: 0.372728\n",
      "Epoch: 3/4... Step: 1700... Loss: 0.337990... Val Loss: 0.388422\n",
      "Epoch: 3/4... Step: 1800... Loss: 0.233356... Val Loss: 0.359596\n",
      "Epoch: 3/4... Step: 1900... Loss: 0.355362... Val Loss: 0.368711\n",
      "Epoch: 3/4... Step: 2000... Loss: 0.216043... Val Loss: 0.348750\n",
      "Epoch: 3/4... Step: 2100... Loss: 0.243876... Val Loss: 0.342326\n",
      "Epoch: 3/4... Step: 2200... Loss: 0.620849... Val Loss: 0.365552\n",
      "Epoch: 3/4... Step: 2300... Loss: 0.546307... Val Loss: 0.341316\n",
      "Epoch: 4/4... Step: 2400... Loss: 0.125147... Val Loss: 0.345773\n",
      "Epoch: 4/4... Step: 2500... Loss: 0.344807... Val Loss: 0.360935\n",
      "Epoch: 4/4... Step: 2600... Loss: 0.181924... Val Loss: 0.347558\n",
      "Epoch: 4/4... Step: 2700... Loss: 0.268330... Val Loss: 0.352380\n",
      "Epoch: 4/4... Step: 2800... Loss: 0.117608... Val Loss: 0.356151\n",
      "Epoch: 4/4... Step: 2900... Loss: 0.176039... Val Loss: 0.346019\n",
      "Epoch: 4/4... Step: 3000... Loss: 0.190940... Val Loss: 0.353586\n",
      "Epoch: 4/4... Step: 3100... Loss: 0.265660... Val Loss: 0.350626\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "net.to(device)\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output = net(inputs)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "#         nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in test_loader:\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                output = net(inputs)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "copguZMtlp15"
   },
   "source": [
    "### Inference on a test review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DFzg-Js6lp15"
   },
   "outputs": [],
   "source": [
    "# negative test review\n",
    "test_review= 'The worst movie I have seen; acting was terrible and I want my money back. \\\n",
    "                    This movie had bad acting and the dialogue was slow.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   2, 241,  17,  10,  25, 105, 113,\n",
      "         13, 368,   3,  10, 177,  54, 297, 143,  11,  17,  67,  78, 113,   3,\n",
      "          2, 408,  13, 602], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "text = preprocess(test_review)\n",
    "indexed = text_pipeline(text)\n",
    "indexed = torch.tensor(indexed, dtype=torch.long).to(device)\n",
    "print(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"negative\" if torch.sigmoid(net(indexed.unsqueeze(0))).item() < 0.5 else \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive test review\n",
    "test_review = 'This movie had the best acting and the dialogue was so good. I loved it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = preprocess(test_review)\n",
    "indexed = text_pipeline(text)\n",
    "indexed = torch.tensor(indexed, dtype=torch.long).to(device)\n",
    "\n",
    "\"negative\" if torch.sigmoid(net(indexed.unsqueeze(0))).item() < 0.5 else \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "075_Sentiment_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
