{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no20hZesHZaV"
      },
      "source": [
        "# 150. PyTorch 다중 분류 손실 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLaR1GXEHZaW"
      },
      "source": [
        "## Categorical Crossentropy\n",
        "    \n",
        "<img src=\"https://gombru.github.io/assets/cross_entropy_loss/softmax_CE_pipeline.png\" width=500 />\n",
        "\n",
        "##  `nn.CrossEntropyLoss`\n",
        "\n",
        "- `nn.LogSoftmax()` 와 `nn.NLLLoss()` 를 단일 class 로 combine한 것.\n",
        "\n",
        "    즉, `nn.CrossEntropyLoss()` = `nn.NLLLoss(nn.LogSoftmax())`\n",
        "\n",
        "### nn.CrossEntropyLoss\n",
        "\n",
        "$$loss(x, class) = -\\log(\\frac{exp(x[class])}{\\sum_{j}{exp(x[j])}}) = -x[class] + log(\\sum_{j}exp(x[j]))$$\n",
        "\n",
        "### nn.LogSoftmax\n",
        "\n",
        "$$LogSoftmax(x_i) = \\log(\\frac{exp(x_i)}{\\sum_{j}{exp(x_j)}})$$\n",
        "\n",
        "### nn.NLLLoss (Negative Log Likelihood Loss)\n",
        "$$l(x, y) = - \\frac{1}{N} \\sum_{1}^{N}l_n$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqaoUMmlHZaX"
      },
      "source": [
        "### 결론적으로, Pytorch 에서는 Cross Entropy Loss 값의 결과를 얻기 위해 2가지 방식이 존재"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M9Sff9roHZaX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RFZQfb9HHZaX"
      },
      "outputs": [],
      "source": [
        "def NLLLoss(logs, targets):\n",
        "    # targets와 동일한 크기의 텐서를 생성하고, 데이터 타입은 float으로 설정\n",
        "    out = torch.zeros_like(targets, dtype=torch.float)\n",
        "    # targets의 길이만큼 반복하여 각 샘플에 대해 처리\n",
        "    for i in range(len(targets)):\n",
        "        # logs[i][targets[i]]를 통해, i번째 샘플의 타겟 클래스에 해당하는 로그 확률을 선택합니다.\n",
        "        out[i] = logs[i][targets[i]]\n",
        "    # out 텐서의 원소들의 합을 취한 후, 음수를 취하고 샘플의 수로 나누어 평균을 구합니다.\n",
        "    # 이는 Negative Log Likelihood Loss를 계산하는 과정입니다.\n",
        "    return -out.sum() / len(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1ePowVHaHZaX"
      },
      "outputs": [],
      "source": [
        "# class label 이 1 인 다중분류\n",
        "# 이 텐서는 모델의 출력으로 볼 수 있으며, 각 요소는 특정 클래스에 속할 확률 또는 점수를 나타냅니다.\n",
        "# 여기서는 10개의 다른 클래스에 대한 점수를 포함하고 있습니다.\n",
        "x = torch.Tensor([[0.8982, 0.805, 0.6393, 0.9983, 0.5731, 0.0469, 0.556, 0.1476, 0.8404, 0.5544]])\n",
        "\n",
        "# LongTensor는 정수형 텐서를 나타내며, 여기서는 클래스 레이블이 '1'임을 나타냅니다.\n",
        "y = torch.LongTensor([1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEczqzIDHZaX"
      },
      "source": [
        "## 방법 1\n",
        "### nn.CrossEntropyLoss 함수 사용\n",
        "\n",
        "-  `NNLLoss(Log(Softmax))`\n",
        "\n",
        "$-x[class] + log(\\sum_{j}exp(x[j]))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hWiDO-2HZaX",
        "outputId": "9b799b63-9b4b-4269-e7aa-0193c385db4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.1438)\n"
          ]
        }
      ],
      "source": [
        "# torch.nn.CrossEntropyLoss를 인스턴스화합니다.\n",
        "# 이 손실 함수는 소프트맥스와 NLLLoss(Negative Log Likelihood Loss)를 결합한 것입니다.\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# 손실 함수를 사용하여 예측값 x와 실제 레이블 y 사이의 크로스 엔트로피 손실을 계산합니다.\n",
        "# CrossEntropyLoss는 내부적으로 x에 대해 소프트맥스 함수를 적용하여 확률 분포를 얻고,\n",
        "# 이 확률 분포와 실제 레이블 y 사이의 NLLLoss를 계산합니다.\n",
        "loss = cross_entropy_loss(x, y)\n",
        "\n",
        "# 계산된 손실값 출력\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1kiqyWJHZaY"
      },
      "source": [
        "## 방법 2\n",
        "### nn.LogSoftmax + NLLLoss\n",
        "\n",
        "- LogSoftmax : Log + Softmax - 두 함수를 따로 적용한 것 보다 수학적 안정성이 좋다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8Wr5IYcHZaY",
        "outputId": "7536a836-b149-4a15-c9ec-f0b2e1598b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.1438)\n"
          ]
        }
      ],
      "source": [
        "# torch.nn.LogSoftmax를 인스턴스화합니다.\n",
        "# LogSoftmax는 입력 텐서에 대해 소프트맥스 함수를 적용한 후, 그 결과의 로그 값을 반환\n",
        "# 'dim=1' 매개변수는 소프트맥스 함수가 적용될 차원\n",
        "# 여기서는 각 샘플의 클래스 점수에 대해 소프트맥스 적용\n",
        "log_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "# log_softmax를 사용하여 x에 대한 로그 소프트맥스 값을 계산\n",
        "# 이 결과는 각 클래스에 대한 로그 확률을 나타냅니다.\n",
        "x_log = log_softmax(x)\n",
        "\n",
        "# 앞서 정의한 NLLLoss 함수를 사용하여,\n",
        "# 로그 소프트맥스 값(x_log)과 실제 레이블(y) 사이의 음의 로그 가능도 손실(NLL Loss)을 계산합니다.\n",
        "# NLLLoss는 모델의 예측 로그 확률과 실제 레이블 간의 차이를 측정합니다.\n",
        "# 여기서는 모델의 예측 로그 확률(x_log)이 더 정확할수록, 즉 실제 레이블에 해당하는 로그 확률 값이 높을수록 손실이 줄어듭니다.\n",
        "loss = NLLLoss(x_log, y)\n",
        "\n",
        "# 계산된 손실값 출력\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz6x4rupHZaY",
        "outputId": "02b50057-4135-49b1-824d-befaea8285e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.1438)\n"
          ]
        }
      ],
      "source": [
        "# torch.nn.NLLLoss()를 인스턴스화하고, 계산된 로그 소프트맥스 값(x_log)과\n",
        "# 실제 레이블(y)을 사용하여 음의 로그 가능도 손실(Negative Log Likelihood Loss)을 계산합니다.\n",
        "loss = nn.NLLLoss()(x_log, y)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dR22avpHZaY"
      },
      "source": [
        "### 사용상의 주의 사항\n",
        "\n",
        "#### nn.CrossEntropyLoss 를 사용할 경우\n",
        "-  nn.CrossEntropyLoss 내에 softmax 함수가 포함되어 있으므로 Neural Network의 마지막 activation 함수로 Softmax를 지정 않고, logit 만 출력한다.  \n",
        "\n",
        "#### nn.NLLLoss 를 사용할 경우\n",
        "- nn.NLLLoss 는 입력으로 확률 분포가 와야 하므로, Neural Network의 마지막 actiovation 함수로 LogSoftmax를 지정 한다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CrossEntropyLoss 인스턴스를 생성합니다. 이 손실 함수는 소프트맥스 함수를 적용한 뒤,\n",
        "# 크로스 엔트로피 손실을 계산합니다.\n",
        "cross_entropyloss = nn.CrossEntropyLoss()\n",
        "\n",
        "# LogSoftmax 인스턴스를 생성합니다. 이 함수는 입력 텐서에 대해 로그 소프트맥스 함수를 적용합니다.\n",
        "log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "# NLLLoss(Negative Log Likelihood Loss) 인스턴스를 생성합니다.\n",
        "# 이 손실 함수는 로그 확률에 대한 NLL 손실을 계산합니다.\n",
        "negative_LLLoss = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "1OiZkB3aMb4G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVv9hhGHHZaY",
        "outputId": "3a9f10f1-b08b-4fef-f0d5-67d0c253aed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax index\n",
            "tensor([2.0000, 3.1000, 1.0000]) tensor([0, 2, 1])\n",
            "tensor([2.5000, 3.1000, 3.1000]) tensor([2, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "# 실제 레이블을 나타내는 텐서를 생성합니다.\n",
        "Y = torch.tensor([0, 2, 1])\n",
        "\n",
        "# 좋은 예측값을 나타내는 텐서를 생성합니다.\n",
        "# 이 텐서는 각 샘플에 대해 모델이 예측한 클래스 점수를 담고 있습니다.\n",
        "y_pred_good = torch.tensor([[2.0, 1.0, 0.1], [0.5, 1.0, 3.1], [0.3, 1.0, 0.1]])\n",
        "\n",
        "# 나쁜 예측값을 나타내는 텐서를 생성합니다.\n",
        "y_pred_bad  = torch.tensor([[0.1, 1.0, 2.5], [3.1, 1.0, 0.5], [3.1, 0.1, 2.5]])\n",
        "\n",
        "print(\"argmax index\")\n",
        "# torch.max 함수를 사용하여 y_pred_good의 각 샘플에 대해 가장 높은 점수를 가진 클래스의 인덱스를 얻습니다.\n",
        "_, pred1 = torch.max(y_pred_good, 1)\n",
        "print(_, pred1)\n",
        "\n",
        "# torch.max 함수를 사용하여 y_pred_bad의 각 샘플에 대해 가장 높은 점수를 가진 클래스의 인덱스를 얻습니다.\n",
        "_, pred2 = torch.max(y_pred_bad, 1)\n",
        "print(_, pred2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi1zhUJkHZaZ",
        "outputId": "0e89bd17-688c-4da3-ed7f-06722ea619ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nn.CrossEntropyLoss() 사용 결과 :\n",
            "0.41337862610816956 2.973893404006958\n",
            "\n",
            "nn.LogSoftmax() + nn.NLLLoss() 사용 결과 :\n",
            "0.41337862610816956 2.973893404006958\n"
          ]
        }
      ],
      "source": [
        "# CrossEntropy Loss\n",
        "# nn.CrossEntropyLoss()는 내부적으로 로그 소프트맥스와 NLLLoss를 결합한 것으로,\n",
        "# 좋은 예측과 나쁜 예측에 대한 크로스 엔트로피 손실을 계산합니다.\n",
        "loss_good = cross_entropyloss(y_pred_good, Y)\n",
        "loss_bad = cross_entropyloss(y_pred_bad, Y)\n",
        "print('nn.CrossEntropyLoss() 사용 결과 :')\n",
        "print(loss_good.item(), loss_bad.item())\n",
        "print()\n",
        "\n",
        "# NLLLoss\n",
        "# 먼저 nn.LogSoftmax()를 사용하여 로그 소프트맥스 값을 계산한 후,\n",
        "# nn.NLLLoss()로 네거티브 로그 우도 손실(NLLLoss)을 계산합니다.\n",
        "m1 = log_softmax(y_pred_good)\n",
        "m2 = log_softmax(y_pred_bad)\n",
        "\n",
        "loss_good = negative_LLLoss(m1, Y)\n",
        "loss_bad = negative_LLLoss(m2, Y)\n",
        "print('nn.LogSoftmax() + nn.NLLLoss() 사용 결과 :')\n",
        "print(loss_good.item(), loss_bad.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OmbHJUsHZaZ"
      },
      "source": [
        "### forward method 의 return value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4PsDIWY9HZaZ"
      },
      "outputs": [],
      "source": [
        "class Ex_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(2, 8)\n",
        "        self.linear2 = nn.Linear(8, 3)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward1(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        # CrossEntropyLoss를 사용할 경우\n",
        "        # no log softmax at the end\n",
        "        return x\n",
        "\n",
        "    def forward2(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        # NLLLoss 를 사용할 경우\n",
        "        # log softmax 를 output 에 사용\n",
        "        x = self.log_softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_NUNjbqHZaZ",
        "outputId": "6f432b06-9567-4cff-b808-357ae8967939"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[20., 10.],\n",
              "        [10., 30.],\n",
              "        [10.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model = Ex_NN()\n",
        "\n",
        "x = torch.FloatTensor([[20, 10], [10, 30], [10, 1]])\n",
        "y = torch.tensor([0, 2, 1])\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3wy9jKwHZaZ"
      },
      "source": [
        "- CrossEntropyLoss 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZJ3onH6HZaZ",
        "outputId": "3c1b1f55-8caf-402e-a803-e6a6eddde76c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.5752, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "logits = model.forward1(x)\n",
        "print(loss(logits, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqpfbuqnHZaZ"
      },
      "source": [
        "- Negative Log Likelihood Loss 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDMl6cFsHZaZ",
        "outputId": "2fbabd42-7566-4760-f0ad-1b70a5908476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.5752, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "loss = nn.NLLLoss()\n",
        "prob = model.forward2(x)\n",
        "print(loss(prob, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FxVIEexHZaZ"
      },
      "source": [
        "### category 분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgl3zLfwHZaa",
        "outputId": "b955fdd7-5e41-4b05-b40b-d3299e9ff77e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-9.8463e+00, -5.7576e-05, -1.2279e+01],\n",
            "        [-1.2482e+01, -3.9339e-06, -1.5843e+01],\n",
            "        [-3.7094e+00, -3.6579e-02, -4.4719e+00]],\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZaAHK_gHZaa",
        "outputId": "325745fd-06d9-4837-bf01-43975a63f51f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# prob는 모델의 출력 확률 분포를 나타내는 텐서입니다.\n",
        "\n",
        "# torch.argmax 함수는 주어진 차원(axis)에 대해 최대값을 갖는 인덱스를 반환합니다.\n",
        "# 여기서 axis=-1은 텐서의 마지막 차원을 의미합니다. 다중 클래스 분류 문제에서는\n",
        "# 각 샘플의 예측된 클래스 인덱스를 찾는 데 사용됩니다.\n",
        "torch.argmax(prob, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5O6-sDTHZaa",
        "outputId": "f89d272f-e476-4c3c-a5d8-84aae1a3dcc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(\n",
              "values=tensor([-5.7576e-05, -3.9339e-06, -3.6579e-02], grad_fn=<MaxBackward0>),\n",
              "indices=tensor([1, 1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# `torch.max` 함수는 주어진 텐서에서 최대 값을 찾습니다.\n",
        "# 이 함수는 두 개의 반환값을 가집니다: 최대 값과 해당 값의 인덱스입니다.\n",
        "# `prob` 변수는 특정 확률 분포 또는 점수가 포함된 텐서를 가정합니다.\n",
        "# `axis=-1` 매개변수는 최대 값을 찾을 차원을 지정합니다.\n",
        "# `-1`은 텐서의 마지막 차원을 의미합니다. 이는 다차원 텐서에서 각 벡터별로 최대 값을 찾는 데 사용됩니다.\n",
        "torch.max(prob, axis=-1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}