{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCEP3DUjiJFq"
      },
      "source": [
        "# 220. CNN Basic\n",
        "\n",
        "## Convolution\n",
        "\n",
        "<img src='conv_layer.gif' height=60% width=60% />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fcm5VLzgiJFr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v96JMtkoiJFr"
      },
      "source": [
        "<img src = \"convolution.JPG\" width = 500, align = \"center\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzxkYba0iJFr",
        "outputId": "817cfdca-d625-4432-a166-540610acbc14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=0, bias=False)\n",
        "conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVop0OTiiJFs"
      },
      "source": [
        "- <code>nn.Conv2d</code> 의 parameter 는 random 하게 초기화되고, 훈련 과정에서 학습됨.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtWk8sB8iJFs",
        "outputId": "380c20b5-9766-4389-82ba-996f5d805edc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[[[ 0.3640, -0.2951],\n",
              "                        [-0.0612,  0.1379]]]]))])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "conv.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H863EE28iJFs"
      },
      "source": [
        "- 간단한 계산 예시를 위해 kernel parameter 값을 임의로 수정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqtA1CpNiJFs",
        "outputId": "fde947d2-5af8-45a2-c231-c2cbc490bfda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[[[1., 2.],\n",
              "                        [3., 4.]]]]))])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "conv.state_dict()['weight'][0][0] = \\\n",
        "                        torch.tensor([[1., 2.],\n",
        "                                      [3., 4.]])\n",
        "conv.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C9EonNciJFs"
      },
      "source": [
        "- sample image를 4x4 크기로 생성  \n",
        "\n",
        "- ``torch.nn`` 은 미니 배치(mini-batch)만 지원하므로, `nnConv2D` 는 `nSamples x nChannels x Height x Width` 의\n",
        "    4차원 Tensor를 입력으로 한다. 하나의 샘플만 있다면, `input.unsqueeze(0)` 을 사용해서 가짜 차원을 추가한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAb14ur1iJFs",
        "outputId": "a7b7181a-6c18-4248-908e-3e58fbce4f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[1. 0. 1. 0.]\n",
            "   [0. 1. 1. 0.]\n",
            "   [1. 0. 1. 0.]\n",
            "   [1. 0. 1. 1.]]]]\n"
          ]
        }
      ],
      "source": [
        "sample_image = torch.tensor([\n",
        "                             [1, 0, 1, 0],\n",
        "                             [0, 1, 1, 0],\n",
        "                             [1, 0, 1, 0],\n",
        "                             [1, 0, 1, 1]], dtype=torch.float)\n",
        "\n",
        "sample_image = sample_image.unsqueeze(0).unsqueeze(0)\n",
        "print(sample_image.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avOdFJwaiJFs"
      },
      "source": [
        "- convolution 결과"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1YRez3DiJFt",
        "outputId": "11a59b7d-68a8-4062-b312-646dcbcf8c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 3, 3])\n",
            "\n",
            "[[[[5. 9. 4.]\n",
            "   [5. 7. 4.]\n",
            "   [4. 6. 8.]]]]\n"
          ]
        }
      ],
      "source": [
        "z = conv(sample_image)\n",
        "\n",
        "print(z.shape)\n",
        "print()\n",
        "print(z.detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE36xfF2iJFt"
      },
      "source": [
        "### Convolutional Layer 의 output size 계산\n",
        "\n",
        "<img src = \"padding.JPG\" width = 500, align = \"center\">\n",
        "\n",
        "$$ \\text{output width} = \\lfloor\\frac{W - F_w + 2P}{S_w} + 1\\rfloor  $$\n",
        "$$ \\text{output height} = \\lfloor\\frac{H - F_h + 2P}{S_h} + 1 \\rfloor$$\n",
        "$W - \\text{input width}$  \n",
        "$F_{w,h} - \\text{filter width/height}$  \n",
        "$S_{w,h} - \\text{stride width/height}$  \n",
        "$P - padding$\n",
        "\n",
        "`conv output size = (input size - filter size + 2 * padding size) / stride size + 1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CfFENLlxiJFt"
      },
      "outputs": [],
      "source": [
        "def output_size(W, F, P, S, poolsize=1):\n",
        "    size = (W - F + 2*P)/S + 1\n",
        "    return size if poolsize == 1 else size / poolsize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO3-mz91iJFt",
        "outputId": "92e508b7-4df9-44d1-e604-b5beb8858c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 3, 3])\n",
            "[[[[0. 1. 2.]\n",
            "   [3. 4. 5.]\n",
            "   [6. 7. 8.]]]]\n"
          ]
        }
      ],
      "source": [
        "image1 = torch.tensor([\n",
        "                       [0, 1, 2],\n",
        "                       [3, 4, 5],\n",
        "                       [6, 7, 8]], dtype=torch.float)\n",
        "\n",
        "image1 = image1.unsqueeze(0).unsqueeze(0)\n",
        "print(image1.size())\n",
        "print(image1.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QulQGhywiJFt"
      },
      "source": [
        "- no padding 의 경우 output image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmcxkXnmiJFt",
        "outputId": "ab74e1d6-d754-4fbf-e4c5-212d4a219fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 image size = torch.Size([1, 1, 3, 3])\n",
            "convolution 후 image size = torch.Size([1, 1, 2, 2])\n",
            "Convolution 출력 data = [[[[19. 25.]\n",
            "   [37. 43.]]]]\n",
            "\n",
            "출력 Hight =  2\n",
            "출력 Width =  2\n"
          ]
        }
      ],
      "source": [
        "F = 2\n",
        "S = 1\n",
        "P = 0\n",
        "\n",
        "conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=F, stride=S, bias=False)\n",
        "\n",
        "conv1.state_dict()['weight'][0][0] = torch.tensor([[0., 1.], [2., 3.]])\n",
        "\n",
        "z1 = conv1(image1)\n",
        "\n",
        "print(\"입력 image size =\", image1.size())\n",
        "print(\"convolution 후 image size =\", z1.size())\n",
        "print(\"Convolution 출력 data =\", z1.detach().numpy())\n",
        "print()\n",
        "\n",
        "H = image1.size()[2]\n",
        "W = image1.size()[3]\n",
        "print(\"출력 Hight = \", (H - F + 2*P) // S + 1)\n",
        "print(\"출력 Width = \", (W - F + 2*P) // S + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cj7gW65iJFt",
        "outputId": "6c29e3cf-db6a-42b6-c86f-daf2631f0397"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "output_size(W, F, P, S, poolsize=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq7ZJzygiJFt"
      },
      "source": [
        "- padding=1 인 경우의 output image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFgdmsWEiJFt",
        "outputId": "149c007a-1a00-417b-ee66-121d3372074c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 image size = torch.Size([1, 1, 3, 3])\n",
            "convolution 후 image size = torch.Size([1, 1, 4, 4])\n",
            "Convolution 출력 data = [[[[ 0.  3.  8.  4.]\n",
            "   [ 9. 19. 25. 10.]\n",
            "   [21. 37. 43. 16.]\n",
            "   [ 6.  7.  8.  0.]]]]\n",
            "\n",
            "출력 Hight =  4\n",
            "출력 Width =  4\n"
          ]
        }
      ],
      "source": [
        "F = 2\n",
        "S = 1\n",
        "P = 1\n",
        "\n",
        "conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=F, stride=S, padding=P, bias=False)\n",
        "\n",
        "conv2.state_dict()['weight'][0][0] = torch.tensor([[0., 1.], [2., 3.]])\n",
        "\n",
        "z2 = conv2(image1)\n",
        "\n",
        "print(\"입력 image size =\", image1.size())\n",
        "print(\"convolution 후 image size =\", z2.size())\n",
        "print(\"Convolution 출력 data =\", z2.detach().numpy())\n",
        "print()\n",
        "\n",
        "print(\"출력 Hight = \", (H - F + 2*P) // S + 1)\n",
        "print(\"출력 Width = \", (W - F + 2*P) // S + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMPpsjiyiJFu",
        "outputId": "d414f6c1-0c0e-45c3-eb04-287fb8b8b11d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "output_size(W, F, P, S, poolsize=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7uT4CCtiJFu"
      },
      "source": [
        "## Pooling\n",
        "\n",
        "- Max Pooling - <code>torch.nn.MaxPool2d</code>  \n",
        "- Average Pooling - <code>torch.nn.AvgPool2d</code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOTiIlJJiJFu",
        "outputId": "a7ed4aaa-05fc-4e08-fff1-165dbd99c793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 4])\n",
            "torch.Size([1, 1, 4, 4])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1., 1., 1., 4.],\n",
              "          [2., 6., 5., 8.],\n",
              "          [3., 2., 1., 0.],\n",
              "          [1., 1., 3., 5.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# 4x4 크기의 2D 텐서를 생성합니다. 이 텐서는 예를 들어 하나의 그레이스케일 이미지를 나타낼 수 있습니다.\n",
        "image2 = torch.tensor([\n",
        "                       [1, 1, 1, 4],\n",
        "                       [2, 6, 5, 8],\n",
        "                       [3, 2, 1, 0],\n",
        "                       [1, 1, 3, 5]], dtype=torch.float)\n",
        "\n",
        "# image2 텐서의 형태를 출력합니다. 결과는 [4, 4]로, 4x4 크기의 2D 텐서임을 나타냅니다.\n",
        "print(image2.shape)\n",
        "\n",
        "# unsqueeze(0)을 사용하여 첫 번째 차원(배치 차원)을 추가합니다.\n",
        "# 두 번째 unsqueeze(0) 호출로 채널 차원을 추가합니다.\n",
        "# 이렇게 함으로써 2D 이미지 텐서를 [배치 크기, 채널 수, 높이, 너비] 형태의 4D 텐서로 변환합니다.\n",
        "image2 = image2.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "# 변경된 image2 텐서의 형태를 출력합니다. 결과는 [1, 1, 4, 4]로,\n",
        "# 1개의 배치에 속한 1개의 채널을 가진 4x4 크기의 이미지임을 나타냅니다.\n",
        "print(image2.shape)\n",
        "\n",
        "# 변환된 image2 텐서를 출력합니다.\n",
        "image2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWkuUusgiJFu",
        "outputId": "a36c4933-ae1a-4254-d5b0-9a81a7f34943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[6., 8.],\n",
            "          [3., 5.]]]])\n"
          ]
        }
      ],
      "source": [
        "# 2x2 크기의 커널을 사용하고, stride를 2로 설정한 MaxPooling을 정의합니다.\n",
        "max1 = torch.nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "# MaxPooling을 이미지에 적용하고 2x2 영역에서 최대값을 선택하여 이미지의 크기를 줄입니다.\n",
        "print(max1(image2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BasulWKaiJFu",
        "outputId": "9f783b96-e931-4fb4-c104-9daf21a049aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[2.5000, 4.5000],\n",
            "          [1.7500, 2.2500]]]])\n"
          ]
        }
      ],
      "source": [
        "# 2x2 크기의 커널을 사용하고, stride를 2로 설정한 평균 풀링(Average Pooling)을 정의합니다.\n",
        "max2 = torch.nn.AvgPool2d(2, stride=2)\n",
        "\n",
        "# 평균 풀링을 이미지에 적용하고 2x2 영역의 평균값을 계산하여 이미지의 크기를 줄입니다.\n",
        "print(max2(image2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8deBHA6iJFu"
      },
      "source": [
        "### Multiple Channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkeP6hQjiJFv",
        "outputId": "8cecb58c-0c4e-4327-cf80-6ef63012ed97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.6541,  0.4197,  0.5572],\n",
            "          [ 0.2552,  0.8417,  0.5572],\n",
            "          [ 0.5572,  0.3520,  0.7217]],\n",
            "\n",
            "         [[ 0.0839, -0.3128,  0.2103],\n",
            "          [-0.2599, -0.2494,  0.2103],\n",
            "          [ 0.2103, -0.3863,  0.1574]],\n",
            "\n",
            "         [[-0.4985, -1.2642, -0.5449],\n",
            "          [-0.8941, -0.9760, -0.5449],\n",
            "          [-0.5449, -0.8476, -0.9151]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 입력 채널이 1개, 출력 채널이 3개인 합성곱 레이어\n",
        "# kernel_size는 커널(또는 필터)의 크기를 2x2로 설정합니다.\n",
        "# bias=False는 합성곱 연산 시 편향(bias)을 추가하지 않겠다는 의미입니다.\n",
        "conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=2, bias=False)\n",
        "\n",
        "# 이 예제에서는 출력 채널이 3개이므로, 3개의 다른 커널이 각각 적용되어 3개의 특징 맵을 출력\n",
        "result = conv(sample_image)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQJBRbayiJFv"
      },
      "source": [
        "### Flatten\n",
        "\n",
        "- ``torch.flatten(x, 1)``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "semCA2JTiJFv",
        "outputId": "a11dec40-7f54-4e5f-e43b-db75c72ed9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 2])\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n"
          ]
        }
      ],
      "source": [
        "t = torch.tensor([\n",
        "                   [[1, 2],\n",
        "                   [3, 4]],\n",
        "                  [[5, 6],\n",
        "                   [7, 8]]])\n",
        "print(t.shape)\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_2LZ6qTiJFv",
        "outputId": "f42e820f-1b9a-413c-f7ce-db42e25b1968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4])\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n"
          ]
        }
      ],
      "source": [
        "# torch.flatten 함수는 텐서를 평탄화합니다. 여기서 시작 차원(start_dim)을 1로 설정합니다.\n",
        "# 이는 첫 번째 차원(배치 크기)을 유지한 채 나머지 차원을 평탄화하여 2차원 텐서를 생성합니다.\n",
        "# 예를 들어, [배치 크기, 채널 수, 높이, 너비] 형태의 텐서는 [배치 크기, 채널 수*높이*너비]로 평탄화됩니다.\n",
        "flattened_t = torch.flatten(t, 1)\n",
        "\n",
        "print(flattened_t.shape)\n",
        "print(flattened_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InHyZh_IiJFv",
        "outputId": "cf9242c9-6658-4302-ee97-365f15dfd6cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4])\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n"
          ]
        }
      ],
      "source": [
        "# flatten() 과 동일 result\n",
        "# t를 [배치 크기, 나머지 모든 요소의 수]의 2차원 텐서로 변환합니다.\n",
        "# 예를 들어, [배치 크기, 채널 수, 높이, 너비] 형태의 텐서는 [배치 크기, 채널 수*높이*너비]로 변환됩니다.\n",
        "reshaped_t = t.view(t.size()[0], -1)\n",
        "print(reshaped_t.shape)\n",
        "print(reshaped_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M34Az_rSiJFv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}